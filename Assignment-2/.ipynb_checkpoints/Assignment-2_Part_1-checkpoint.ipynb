{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Multi-layer Feed Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions to assignment #2 by K. Sai Somanath, 18MCMT28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "## Extracting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions to read and extract data in desired format\n",
    "def read(dataset = \"training\", path = \".\"):\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        print(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        _, __, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows * cols)\n",
    "\n",
    "    get_img = lambda index: (lbl[index], img[index])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_img(i)\n",
    "\n",
    "def show(image):\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image.reshape(28, 28), cmap=mpl.cm.gray)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the images\n",
    "TRAIN = read('training', 'MNIST'); TEST = read('testing', 'MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = []; lbl_train = []\n",
    "img_test = []; lbl_test = []\n",
    "\n",
    "for temp in TRAIN:\n",
    "    img_train.append(temp[1])\n",
    "    lbl_train.append(temp[0])\n",
    "\n",
    "for temp in TEST:\n",
    "    img_test.append(temp[1])\n",
    "    lbl_test.append(temp[0])\n",
    "\n",
    "img_train = np.array(img_train); lbl_train = np.array(lbl_train)\n",
    "img_test = np.array(img_test); lbl_test = np.array(lbl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "A look at a random image to make sure everything went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(img_test[8])\n",
    "print(lbl_test[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the labels of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_train = np.eye(10)[lbl_train]\n",
    "lbl_test = np.eye(10)[lbl_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the labels in the trainig smaplemhave been one hot encoded. Instaed of having a single digit representing the class name, we instead use a vector of size 10 to represent the class of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_test[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img_train / 255\n",
    "img_test = img_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLFFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "        \n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        return Sigmoid.activation(z) * (1 - Sigmoid.activation(z))\n",
    "    \n",
    "class MSE:\n",
    "    def __init__(self, activation_fn=None):\n",
    "        self.activation_fn = activation_fn\n",
    "            \n",
    "    def activation(self, z):\n",
    "        return self.activation_fn.activation(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(y_true, y_pred):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    def delta(self, y_true, y_pred):\n",
    "        return self.derivative(y_true, y_pred) * self.activation_fn.derivative(y_pred)\n",
    "    \n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, dimensions, activation_fns):\n",
    "        self.dimensions = dimensions\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "        self.weights = {}\n",
    "        self.bais = {}\n",
    "        self.activations = {}\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.weights[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.bais[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activation_fns[i]\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        deepcopy_method = self.__deepcopy__\n",
    "        self.__deepcopy__ = None\n",
    "        cp = deepcopy(self, memo)\n",
    "        self.__deepcopy__ = deepcopy_method\n",
    "        # custom treatments\n",
    "        cp.weights = {}; cp.bais = {}\n",
    "        for i in range(cp.n_layers - 1):\n",
    "            cp.weights[i + 1] = np.random.randn(cp.dimensions[i], cp.dimensions[i + 1]) / np.sqrt(cp.dimensions[i])\n",
    "            cp.bais[i + 1] = np.zeros(cp.dimensions[i + 1])\n",
    "\n",
    "        return cp\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        z = {}\n",
    "        activated = {1: x}\n",
    "        for i in range(1, self.n_layers):\n",
    "            z[i + 1] = np.dot(activated[i], self.weights[i]) + self.bais[i]\n",
    "            activated[i + 1] = self.activations[i + 1].activation(z[i + 1])\n",
    "        return z, activated\n",
    "    \n",
    "    def back_propagation(self, z, a, y_true):\n",
    "        delta = self.loss.delta(y_true, a[self.n_layers])\n",
    "        partial_derivative = np.dot(a[self.n_layers - 1].T, delta)\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (partial_derivative, delta)\n",
    "        }\n",
    "\n",
    "        for i in reversed(range(2, self.n_layers)):\n",
    "            delta = np.dot(delta, self.weights[i].T) * self.activations[i].derivative(z[i])\n",
    "            partial_derivative = np.dot(a[i - 1].T, delta)\n",
    "            update_params[i - 1] = (partial_derivative, delta)\n",
    "\n",
    "        for key, values in update_params.items():\n",
    "            self.update_fn(key, values[0], values[1])\n",
    "        \n",
    "    def update_fn(self, key, partial_derivative, delta):\n",
    "        self.weights[key] -= self.learning_rate * partial_derivative\n",
    "        self.bais[key] -= self.learning_rate * np.mean(delta, 0)\n",
    "\n",
    "    def learn(self, x, y_true, loss, epochs, batch_size, learning_rate):\n",
    "        self.loss = loss(self.activations[self.n_layers])\n",
    "        self.learning_rate = learning_rate\n",
    "        for i in range(epochs):\n",
    "            seed = np.arange(x.shape[0])\n",
    "            np.random.shuffle(seed)\n",
    "            x_ = x[seed]\n",
    "            y_ = y_true[seed]\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                k = j * batch_size\n",
    "                l = (j + 1) * batch_size\n",
    "                z, a = self.feed_forward(x_[k:l])\n",
    "                self.back_propagation(z, a, y_[k:l])\n",
    "            _, _a = self.feed_forward(x)\n",
    "            print(\"Epoch:\", i + 1, \"Loss:\", self.loss.loss(y_true, _a[self.n_layers]), end='\\r')\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.feed_forward(x)\n",
    "        return a[self.n_layers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class allows us to create a network pf arbitary size and supports ReLU and Sigmoid as activations functions. \n",
    "\n",
    "Cross-validation is used to determine the better model for this problem, the value of k is 5, i.e. we create 5 splits of the data set. We then will use the results obtained model contructed in each fold to find the better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(img_train)\n",
    "Y = np.copy(lbl_train)\n",
    "\n",
    "# Creating the 5 fold cross-validation \n",
    "kf = KFold(n_splits=5)\n",
    "\"\"\"\n",
    "We create a new model in each fold and train on 4 splits while we hold the 5th split for testing. We repeat this \n",
    "process for all the combinations. We store the accuracy for each split and discard the model. The model with \n",
    "better accuracy will the better suited for our problem.\n",
    "\"\"\"\n",
    "\n",
    "# Define the models\n",
    "\"\"\"This neural network has 3 layers, 784 input neurons, 100 in the hidden layer, and 10 in the output layer.\n",
    "We use a learning rate of 0.01 and a modest 200 epochs to get a rough idea aboyt the model\"\"\"\n",
    "nn1 = NeuralNetwork((784, 100, 10), (Relu, Sigmoid))\n",
    "\n",
    "\"\"\"This neural network has 4 layers, 784 input neurons, 64, 64 in the hidden layers, and 10 in the output layer.\n",
    "We use a learning rate of 0.1 and a modest 200 epochs to get a rough idea aboyt the model\"\"\"\n",
    "nn2 = NeuralNetwork((784, 64, 64, 10), (Sigmoid, Sigmoid, Sigmoid))\n",
    "\n",
    "## The error array is used to hold the errors made in each fold.\n",
    "e1 = []; e2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "i = 1\n",
    "for train, test in kf.split(X):\n",
    "    x = X[train]; y = Y[train]\n",
    "    x_ = X[test]; y_ = Y[test]\n",
    "    print(\"Split: \", i)\n",
    "    i += 1\n",
    "    nn1.learn(x, y, MSE, 100, 128, 0.01)\n",
    "    print()\n",
    "    y_pred = np.argmax(nn1.predict(x_), axis=1)\n",
    "    y_true = np.argmax(y_, axis=1)\n",
    "    e1.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The error rate of model 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "i = 1\n",
    "for train, test in kf.split(X):\n",
    "    x = X[train]; y = Y[train]\n",
    "    x_ = X[test]; y_ = Y[test]\n",
    "    print(\"Split: \", i)\n",
    "    i += 1\n",
    "    nn2.learn(x, y, MSE, 100, 128, 0.01)\n",
    "    y_pred = np.argmax(nn2.predict(x_), axis=1)\n",
    "    y_true = np.argmax(y_, axis=1)\n",
    "    e2.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The error rate of model 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = np.array(e1); e2 = np.array(e2)\n",
    "print(e1.mean(), e2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model two just barely performs better. We therefore choose, the secnod model to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Digit classifier\n",
    "We have determined that the neural network #2 is the better one to perform classification. We will now train it to on the entire dataset.\n",
    "\n",
    "We use each pixel as a feature to train the network. This results in a network that takes $28\\times28$ number of pixels as input. We have two hidden layers each with 64 nuerons, activated by a Sigmoid function. Lastly, the output layer has 10 neuron which determine the class label of a given input image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the better model\n",
    "if e1.mean() < e2.mean(): \n",
    "    nn_simple = deepcopy(nn1)\n",
    "else:\n",
    "    nn_simple = deepcopy(nn2)\n",
    "\n",
    "# Train the network\n",
    "nn_simple.learn(img_train, lbl_train, MSE, 500, 128, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some metrics to guage the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Mak epredictions on the test set\n",
    "y_pred = np.argmax(nn_simple.predict(img_test), axis=1)\n",
    "# Get the true labels\n",
    "y_true = np.argmax(lbl_test, axis=1)\n",
    "print(\"Metrics of Performance\")\n",
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred) * 100, \"%\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"\\n\\nConfusion Matrix\\n\")\n",
    "print(confusion_matrix(y_pred, y_true))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"\\n\\nOther metrics\\n\")\n",
    "print(classification_report(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have scored an accuracy of about 98%. From the precision column we note that all classes have high precision. This is also evident from the consfusion matrix. \n",
    "\n",
    "With more epochs, it seems like there is a good chance of overfitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Use KNN classifier to learn hand written digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KNN from sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Perform 1 Nearest Neighbour\n",
    "K = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the input data format for 1-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(lbl_train, axis=1)\n",
    "y_test = np.argmax(lbl_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a 1NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_1nn = KNeighborsClassifier(K, algorithm='brute')\n",
    "clf_1nn.fit(img_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "acc = clf_1nn.score(img_test, y_test)\n",
    "print(\"Accuracy = \", (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn_pred = clf_1nn.predict(img_test)\n",
    "print(\"Metrics of Performance\")\n",
    "print(\"Accuracy: \", accuracy_score(y_true, knn_pred) * 100, \"%\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"\\n\\nConfusion Matrix\\n\")\n",
    "print(confusion_matrix(knn_pred, y_true))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"\\n\\nOther metrics\\n\")\n",
    "print(classification_report(knn_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Oberservations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some general observations**\n",
    "1. Accuracy: The 1NN classifier has obtained better accuracy than the neural network\n",
    "2. Training time: The nearest neighbour takes no time to train, as it is a lazy learner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    "One nearest neighbours is able to distinguish the numbers **(3, 7, 9)**, for which the neural network performed worst. It is clear from the confusion matix that the neural network gets really confused when presented with an instance of the above classes. \n",
    "\n",
    "On the other hand, the 1NN model outshines the neural network in regards with the above stated problem. Futhermore, the model was able to classify, the classes **(0, 1, 6)**, with almost cent percent accuracy.\n",
    "\n",
    "The model faces the most difficulty while looking at images of **(3)** and  **(8)**. We can see from the consfusion matrix that, these were the most frequent mistakes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbour is associated with the dimensionality problem as it will look at all the features before determining the class lable of a point. It will have to look at 784 sized vector for 60,000 times. Only then it can classify a point. \n",
    "\n",
    "The neural network on the other hand would have to do some matix multiplication, which depend on the number of hidden layers. The neural network has learnt some weights, unlike the nearest neighboiur algorithm, which simply stores all the training data and waits until the prediction time, to use them(training data) for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
