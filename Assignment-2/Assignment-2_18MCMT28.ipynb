{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Multi-layer Feed Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions to assignment #2 by K. Sai Somanath, 18MCMT28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "## Extracting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions to read and extract data in desired format\n",
    "def read(dataset = \"training\", path = \".\"):\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        print(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        _, __, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows * cols)\n",
    "\n",
    "    get_img = lambda index: (lbl[index], img[index])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_img(i)\n",
    "\n",
    "def show(image):\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image.reshape(28, 28), cmap=mpl.cm.gray)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the images\n",
    "TRAIN = read('training', 'MNIST'); TEST = read('testing', 'MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = []; lbl_train = []\n",
    "img_test = []; lbl_test = []\n",
    "\n",
    "for temp in TRAIN:\n",
    "    img_train.append(temp[1])\n",
    "    lbl_train.append(temp[0])\n",
    "\n",
    "for temp in TEST:\n",
    "    img_test.append(temp[1])\n",
    "    lbl_test.append(temp[0])\n",
    "\n",
    "img_train = np.array(img_train); lbl_train = np.array(lbl_train)\n",
    "img_test = np.array(img_test); lbl_test = np.array(lbl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "A look at a random image to make sure everything went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "show(img_test[8])\n",
    "print(lbl_test[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the labels of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_train = np.eye(10)[lbl_train]\n",
    "lbl_test = np.eye(10)[lbl_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the labels in the trainig smaplemhave been one hot encoded. Instaed of having a single digit representing the class name, we instead use a vector of size 10 to represent the class of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_test[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLFFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "        \n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        return Sigmoid.activation(z) * (1 - Sigmoid.activation(z))\n",
    "    \n",
    "class MSE:\n",
    "    def __init__(self, activation_fn=None):\n",
    "        self.activation_fn = activation_fn\n",
    "            \n",
    "    def activation(self, z):\n",
    "        return self.activation_fn.activation(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(y_true, y_pred):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    def delta(self, y_true, y_pred):\n",
    "        return self.derivative(y_true, y_pred) * self.activation_fn.derivative(y_pred)\n",
    "    \n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, dimensions, activation_fns):\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "        self.weights = {}\n",
    "        self.bais = {}\n",
    "        self.activations = {}\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.weights[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.bais[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activation_fns[i]\n",
    "            \n",
    "    def feed_forward(self, x):\n",
    "        z = {}\n",
    "        activated = {1: x}\n",
    "        for i in range(1, self.n_layers):\n",
    "            z[i + 1] = np.dot(activated[i], self.weights[i]) + self.bais[i]\n",
    "            activated[i + 1] = self.activations[i + 1].activation(z[i + 1])\n",
    "        return z, activated\n",
    "    \n",
    "    def back_propagation(self, z, a, y_true):\n",
    "        delta = self.loss.delta(y_true, a[self.n_layers])\n",
    "        partial_derivative = np.dot(a[self.n_layers - 1].T, delta)\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (partial_derivative, delta)\n",
    "        }\n",
    "\n",
    "        for i in reversed(range(2, self.n_layers)):\n",
    "            delta = np.dot(delta, self.weights[i].T) * self.activations[i].derivative(z[i])\n",
    "            partial_derivative = np.dot(a[i - 1].T, delta)\n",
    "            update_params[i - 1] = (partial_derivative, delta)\n",
    "\n",
    "        for key, values in update_params.items():\n",
    "            self.update_fn(key, values[0], values[1])\n",
    "        \n",
    "    def update_fn(self, key, partial_derivative, delta):\n",
    "        self.weights[key] -= self.learning_rate * partial_derivative\n",
    "        self.bais[key] -= self.learning_rate * np.mean(delta, 0)\n",
    "\n",
    "    def learn(self, x, y_true, loss, epochs, batch_size, learning_rate):\n",
    "        self.loss = loss(self.activations[self.n_layers])\n",
    "        self.learning_rate = learning_rate\n",
    "        for i in range(epochs):\n",
    "            seed = np.arange(x.shape[0])\n",
    "            np.random.shuffle(seed)\n",
    "            x_ = x[seed]\n",
    "            y_ = y_true[seed]\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                k = j * batch_size\n",
    "                l = (j + 1) * batch_size\n",
    "                z, a = self.feed_forward(x_[k:l])\n",
    "                self.back_propagation(z, a, y_[k:l])\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                _, a = self.feed_forward(x)\n",
    "                print(\"Epoch\", i + 1, \"Loss:\", self.loss.loss(y_true, a[self.n_layers]))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.feed_forward(x)\n",
    "        return a[self.n_layers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class allows us to create a network pf arbitary size and supports ReLU and Sigmoid as activations functions. \n",
    "\n",
    "Cross-validation is used to determine the better model for this problem, the value of k is 5, i.e. we create 5 splits of the data set. We then will use the results obtained model contructed in each fold to find the better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(img_train)\n",
    "Y = np.copy(lbl_train)\n",
    "\n",
    "# Creating the 5 fold cross-validation \n",
    "kf = KFold(n_splits=5)\n",
    "\"\"\"\n",
    "We create a new model in each fold and train on 4 splits while we hold the 5th split for testing. We repeat this \n",
    "process for all the combinations. We store the accuracy for each split and discard the model. The model with \n",
    "better accuracy will the better suited for our problem.\n",
    "\"\"\"\n",
    "\n",
    "# Define the models\n",
    "\"\"\"This neural network has 3 layers, 784 input neurons, 100 in the hidden layer, and 10 in the output layer.\n",
    "We use a learning rate of 0.01 and a modest 200 epochs to get a rough idea aboyt the model\"\"\"\n",
    "nn1 = NeuralNetwork((784, 100, 10), (Sigmoid, Sigmoid))\n",
    "\n",
    "\"\"\"This neural network has 4 layers, 784 input neurons, 64, 64 in the hidden layers, and 10 in the output layer.\n",
    "We use a learning rate of 0.1 and a modest 200 epochs to get a rough idea aboyt the model\"\"\"\n",
    "nn2 = NeuralNetwork((784, 64, 64, 10), (Sigmoid, Sigmoid, Sigmoid))\n",
    "\n",
    "## The error array is used to hold the errors made in each fold.\n",
    "e1 = []; e2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/.virtualenvs/Tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Loss: 0.02207418208672041\n",
      "Epoch 200 Loss: 0.0185524114703845\n",
      "Split:  1\n",
      "Epoch 100 Loss: 0.018316540070854512\n",
      "Epoch 200 Loss: 0.015244878327379679\n",
      "Split:  1\n",
      "Epoch 100 Loss: 0.014882411752147834\n",
      "Epoch 200 Loss: 0.014106201201793175\n",
      "Split:  1\n",
      "Epoch 100 Loss: 0.013916263955573096\n",
      "Epoch 200 Loss: 0.013251018019402873\n",
      "Split:  1\n",
      "Epoch 100 Loss: 0.01517111243308256\n",
      "Epoch 200 Loss: 0.013064052700995976\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split(X):\n",
    "    i = 1\n",
    "    x = X[train]; y = Y[train]\n",
    "    x_ = X[test]; y_ = Y[test]\n",
    "    print(\"Split: \", i)\n",
    "    i += 1\n",
    "    nn1.learn(x, y, MSE, 200, 128, 0.01)\n",
    "    y_pred = np.argmax(nn1.predict(x_), axis=1)\n",
    "    y_true = np.argmax(y_, axis=1)\n",
    "    e1.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.88425,\n",
       " 0.8983333333333333,\n",
       " 0.9075833333333333,\n",
       " 0.9096666666666666,\n",
       " 0.9229166666666667]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/.virtualenvs/Tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Loss: 0.01642738241306958\n",
      "Epoch 200 Loss: 0.015496486477681627\n",
      "Split:  2\n",
      "Epoch 100 Loss: 0.014911084301548173\n",
      "Epoch 200 Loss: 0.01526763052835603\n",
      "Split:  3\n",
      "Epoch 100 Loss: 0.012297334503243895\n",
      "Epoch 200 Loss: 0.012020532603717075\n",
      "Split:  4\n",
      "Epoch 100 Loss: 0.011415938785978432\n",
      "Epoch 200 Loss: 0.012294607753273854\n",
      "Split:  5\n",
      "Epoch 100 Loss: 0.010926281419909993\n",
      "Epoch 200 Loss: 0.010916970340238894\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for train, test in kf.split(X):\n",
    "    x = X[train]; y = Y[train]\n",
    "    x_ = X[test]; y_ = Y[test]\n",
    "    print(\"Split: \", i)\n",
    "    i += 1\n",
    "    nn2.learn(x, y, MSE, 200, 128, 0.01)\n",
    "    y_pred = np.argmax(nn2.predict(x_), axis=1)\n",
    "    y_true = np.argmax(y_, axis=1)\n",
    "    e2.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.89675, 0.8975, 0.9150833333333334, 0.9105833333333333, 0.93]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9045499999999999 0.9099833333333333\n"
     ]
    }
   ],
   "source": [
    "e1 = np.array(e1); e2 = np.array(e2)\n",
    "print(e1.mean(), e2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model two just barely performs better. We therefore choose, the secnod model to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Digit classifier\n",
    "We have determined that the neural network #2 is the better one to perform classification. We will now train it to on the entire dataset.\n",
    "\n",
    "We use each pixel as a feature to train the network. This results in a network that takes $28\\times28$ number of pixels as input. We have two hidden layers each with 64 nuerons, activated by a Sigmoid function. Lastly, the output layer has 10 neuron which determine the class label of a given input image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/.virtualenvs/Tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 Loss: 0.015662487240742894\n",
      "Epoch 200 Loss: 0.016782339588961283\n",
      "Epoch 300 Loss: 0.01466834661138394\n",
      "Epoch 400 Loss: 0.012416464708358409\n",
      "Epoch 500 Loss: 0.011188861745651162\n",
      "Epoch 600 Loss: 0.011494422683120084\n",
      "Epoch 700 Loss: 0.011974595774067396\n",
      "Epoch 800 Loss: 0.011700113850517332\n",
      "Epoch 900 Loss: 0.010191649931897539\n",
      "Epoch 1000 Loss: 0.010433829690871354\n"
     ]
    }
   ],
   "source": [
    "nn_simple = NeuralNetwork((784, 64, 64, 10), (Sigmoid, Sigmoid, Sigmoid))\n",
    "\n",
    "# Train the network\n",
    "nn_simple.learn(img_train, lbl_train, MSE, 1000, 128, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some metrics to guage the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics of Performance\n",
      "Accuracy:  92.43 %\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "\n",
      "[[ 953    1   14    5    1   17   10    2    7    6]\n",
      " [   0 1114    4    4    1    1    2    6    3    7]\n",
      " [   1    4  951   17    9    4    6   23    4    1]\n",
      " [   2    4   12  884    1   28    1    5   17   13]\n",
      " [   5    0    8    1  901    1    6    5    7   29]\n",
      " [   8    1    1   51    1  802   10    0   35   25]\n",
      " [   5    4    9    2   10   14  918    0   12    1]\n",
      " [   2    1   12   11   10    7    0  961    4   16]\n",
      " [   1    6   19   24    3   17    5    3  872   24]\n",
      " [   3    0    2   11   45    1    0   23   13  887]]\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "Other metrics\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.95      1016\n",
      "          1       0.98      0.98      0.98      1142\n",
      "          2       0.92      0.93      0.93      1020\n",
      "          3       0.88      0.91      0.89       967\n",
      "          4       0.92      0.94      0.93       963\n",
      "          5       0.90      0.86      0.88       934\n",
      "          6       0.96      0.94      0.95       975\n",
      "          7       0.93      0.94      0.94      1024\n",
      "          8       0.90      0.90      0.90       974\n",
      "          9       0.88      0.90      0.89       985\n",
      "\n",
      "avg / total       0.92      0.92      0.92     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/.virtualenvs/Tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# Mak epredictions on the test set\n",
    "y_pred = np.argmax(nn_simple.predict(img_test), axis=1)\n",
    "# Get the true labels\n",
    "y_true = np.argmax(lbl_test, axis=1)\n",
    "print(\"Metrics of Performance\")\n",
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred) * 100, \"%\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"\\n\\nConfusion Matrix\\n\")\n",
    "print(confusion_matrix(y_pred, y_true))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"\\n\\nOther metrics\\n\")\n",
    "print(classification_report(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have scored an accuracy of about 92%. From the precision column we note that apart from class 3 and class 9, every other class has high precision. This is also evident from the consfusion matrix. \n",
    "\n",
    "When we look at the loss that the model reported at training time, we notice that it performed it's best by the end of 500 epochs, but it shows some improvement later. We can get a better model, with more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Use KNN classifier to learn hand written digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KNN from sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Perform 1 Nearest Neighbour\n",
    "K = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the input data format for 1-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(lbl_train, axis=1)\n",
    "y_test = np.argmax(lbl_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a 1NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  96.91\n"
     ]
    }
   ],
   "source": [
    "clf_1nn = KNeighborsClassifier(K)\n",
    "clf_1nn.fit(img_train, y_train)\n",
    "acc = clf_1nn.score(img_test, y_test)\n",
    "print(\"Accuracy = \", (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics of Performance\n",
      "Accuracy:  96.91 %\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "\n",
      "[[ 973    0    7    0    0    1    4    0    6    2]\n",
      " [   1 1129    6    1    7    1    2   14    1    5]\n",
      " [   1    3  992    2    0    0    0    6    3    1]\n",
      " [   0    0    5  970    0   12    0    2   14    6]\n",
      " [   0    1    1    1  944    2    3    4    5   10]\n",
      " [   1    1    0   19    0  860    5    0   13    5]\n",
      " [   3    1    2    0    3    5  944    0    3    1]\n",
      " [   1    0   16    7    5    1    0  992    4   11]\n",
      " [   0    0    3    7    1    6    0    0  920    1]\n",
      " [   0    0    0    3   22    4    0   10    5  967]]\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "Other metrics\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       993\n",
      "          1       0.99      0.97      0.98      1167\n",
      "          2       0.96      0.98      0.97      1008\n",
      "          3       0.96      0.96      0.96      1009\n",
      "          4       0.96      0.97      0.97       971\n",
      "          5       0.96      0.95      0.96       904\n",
      "          6       0.99      0.98      0.98       962\n",
      "          7       0.96      0.96      0.96      1037\n",
      "          8       0.94      0.98      0.96       938\n",
      "          9       0.96      0.96      0.96      1011\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_pred = clf_1nn.predict(img_test)\n",
    "print(\"Metrics of Performance\")\n",
    "print(\"Accuracy: \", accuracy_score(y_true, knn_pred) * 100, \"%\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"\\n\\nConfusion Matrix\\n\")\n",
    "print(confusion_matrix(knn_pred, y_true))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"\\n\\nOther metrics\\n\")\n",
    "print(classification_report(knn_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Oberservations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
