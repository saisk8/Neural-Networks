{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Multi-layer Feed Forward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions to assignment #2 by K. Sai Somanath, 18MCMT28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "## Extracting the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions to read and extract data in desired format\n",
    "def read(dataset = \"training\", path = \".\"):\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        print(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        _, __, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows * cols)\n",
    "\n",
    "    get_img = lambda index: (lbl[index], img[index])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in range(len(lbl)):\n",
    "        yield get_img(i)\n",
    "\n",
    "def show(image):\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image.reshape(28, 28), cmap=mpl.cm.gray)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the images\n",
    "TRAIN = read('training', 'MNIST'); TEST = read('testing', 'MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = []; lbl_train = []\n",
    "img_test = []; lbl_test = []\n",
    "\n",
    "for temp in TRAIN:\n",
    "    img_train.append(temp[1])\n",
    "    lbl_train.append(temp[0])\n",
    "\n",
    "for temp in TEST:\n",
    "    img_test.append(temp[1])\n",
    "    lbl_test.append(temp[0])\n",
    "\n",
    "img_train = np.array(img_train); lbl_train = np.array(lbl_train)\n",
    "img_test = np.array(img_test); lbl_test = np.array(lbl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset\n",
    "A look at a random image to make sure everything went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "show(img_test[8])\n",
    "print(lbl_test[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the labels of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_train = np.eye(10)[lbl_train]\n",
    "lbl_test = np.eye(10)[lbl_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the labels in the trainig smaplemhave been one hot encoded. Instaed of having a single digit representing the class name, we instead use a vector of size 10 to represent the class of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_test[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train = img_train / 255\n",
    "img_test = img_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MLFFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        z[z < 0] = 0\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        z[z < 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "        \n",
    "class Sigmoid:\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        return Sigmoid.activation(z) * (1 - Sigmoid.activation(z))\n",
    "    \n",
    "class MSE:\n",
    "    def __init__(self, activation_fn=None):\n",
    "        self.activation_fn = activation_fn\n",
    "            \n",
    "    def activation(self, z):\n",
    "        return self.activation_fn.activation(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(y_true, y_pred):\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative(y_true, y_pred):\n",
    "        return y_pred - y_true\n",
    "\n",
    "    def delta(self, y_true, y_pred):\n",
    "        return self.derivative(y_true, y_pred) * self.activation_fn.derivative(y_pred)\n",
    "    \n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, dimensions, activation_fns):\n",
    "        self.dimensions = dimensions\n",
    "        self.n_layers = len(dimensions)\n",
    "        self.loss = None\n",
    "        self.learning_rate = None\n",
    "        self.weights = {}\n",
    "        self.bais = {}\n",
    "        self.activations = {}\n",
    "        for i in range(self.n_layers - 1):\n",
    "            self.weights[i + 1] = np.random.randn(dimensions[i], dimensions[i + 1]) / np.sqrt(dimensions[i])\n",
    "            self.bais[i + 1] = np.zeros(dimensions[i + 1])\n",
    "            self.activations[i + 2] = activation_fns[i]\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        deepcopy_method = self.__deepcopy__\n",
    "        self.__deepcopy__ = None\n",
    "        cp = deepcopy(self, memo)\n",
    "        self.__deepcopy__ = deepcopy_method\n",
    "        # custom treatments\n",
    "        cp.weights = {}; cp.bais = {}\n",
    "        for i in range(cp.n_layers - 1):\n",
    "            cp.weights[i + 1] = np.random.randn(cp.dimensions[i], cp.dimensions[i + 1]) / np.sqrt(cp.dimensions[i])\n",
    "            cp.bais[i + 1] = np.zeros(cp.dimensions[i + 1])\n",
    "\n",
    "        return cp\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        z = {}\n",
    "        activated = {1: x}\n",
    "        for i in range(1, self.n_layers):\n",
    "            z[i + 1] = np.dot(activated[i], self.weights[i]) + self.bais[i]\n",
    "            activated[i + 1] = self.activations[i + 1].activation(z[i + 1])\n",
    "        return z, activated\n",
    "    \n",
    "    def back_propagation(self, z, a, y_true):\n",
    "        delta = self.loss.delta(y_true, a[self.n_layers])\n",
    "        partial_derivative = np.dot(a[self.n_layers - 1].T, delta)\n",
    "\n",
    "        update_params = {\n",
    "            self.n_layers - 1: (partial_derivative, delta)\n",
    "        }\n",
    "\n",
    "        for i in reversed(range(2, self.n_layers)):\n",
    "            delta = np.dot(delta, self.weights[i].T) * self.activations[i].derivative(z[i])\n",
    "            partial_derivative = np.dot(a[i - 1].T, delta)\n",
    "            update_params[i - 1] = (partial_derivative, delta)\n",
    "\n",
    "        for key, values in update_params.items():\n",
    "            self.update_fn(key, values[0], values[1])\n",
    "        \n",
    "    def update_fn(self, key, partial_derivative, delta):\n",
    "        self.weights[key] -= self.learning_rate * partial_derivative\n",
    "        self.bais[key] -= self.learning_rate * np.mean(delta, 0)\n",
    "\n",
    "    def learn(self, x, y_true, loss, epochs, batch_size, learning_rate):\n",
    "        self.loss = loss(self.activations[self.n_layers])\n",
    "        self.learning_rate = learning_rate\n",
    "        for i in range(epochs):\n",
    "            seed = np.arange(x.shape[0])\n",
    "            np.random.shuffle(seed)\n",
    "            x_ = x[seed]\n",
    "            y_ = y_true[seed]\n",
    "            for j in range(x.shape[0] // batch_size):\n",
    "                k = j * batch_size\n",
    "                l = (j + 1) * batch_size\n",
    "                z, a = self.feed_forward(x_[k:l])\n",
    "                self.back_propagation(z, a, y_[k:l])\n",
    "            _, _a = self.feed_forward(x)\n",
    "            print(\"Epoch:\", i + 1, \"Loss:\", self.loss.loss(y_true, _a[self.n_layers]), end='\\r')\n",
    "    \n",
    "    def predict(self, x):\n",
    "        _, a = self.feed_forward(x)\n",
    "        return a[self.n_layers]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class allows us to create a network pf arbitary size and supports ReLU and Sigmoid as activations functions. \n",
    "\n",
    "Cross-validation is used to determine the better model for this problem, the value of k is 5, i.e. we create 5 splits of the data set. We then will use the results obtained model contructed in each fold to find the better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(img_train)\n",
    "Y = np.copy(lbl_train)\n",
    "\n",
    "# Creating the 5 fold cross-validation \n",
    "kf = KFold(n_splits=5)\n",
    "\"\"\"\n",
    "We create a new model in each fold and train on 4 splits while we hold the 5th split for testing. We repeat this \n",
    "process for all the combinations. We store the accuracy for each split and discard the model. The model with \n",
    "better accuracy will the better suited for our problem.\n",
    "\"\"\"\n",
    "\n",
    "# Define the models\n",
    "\"\"\"This neural network has 3 layers, 784 input neurons, 100 in the hidden layer, and 10 in the output layer.\n",
    "We use a learning rate of 0.01 and a modest 100 epochs to get a rough idea aboyt the model\"\"\"\n",
    "nn1 = NeuralNetwork((784, 100, 10), (Relu, Sigmoid))\n",
    "\n",
    "\"\"\"This neural network has 4 layers, 784 input neurons, 64, 64 in the hidden layers, and 10 in the output layer.\n",
    "We use a learning rate of 0.1 and a modest 100 epochs to get a rough idea aboyt the model\"\"\"\n",
    "nn2 = NeuralNetwork((784, 64, 64, 10), (Sigmoid, Sigmoid, Sigmoid))\n",
    "\n",
    "## The error array is used to hold the errors made in each fold.\n",
    "e1 = []; e2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  1\n",
      "Epoch: 100 Loss: 1.4057382128508965e-05\n",
      "Split:  2\n",
      "Epoch: 100 Loss: 3.318764573361809e-06\n",
      "Split:  3\n",
      "Epoch: 100 Loss: 1.0544756549524098e-06\n",
      "Split:  4\n",
      "Epoch: 100 Loss: 4.900567055966747e-07\n",
      "Split:  5\n",
      "Epoch: 100 Loss: 2.904856635513469e-07\n",
      "CPU times: user 1h 5min 58s, sys: 39min 11s, total: 1h 45min 10s\n",
      "Wall time: 9min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 1\n",
    "for train, test in kf.split(X):\n",
    "    x = X[train]; y = Y[train]\n",
    "    x_ = X[test]; y_ = Y[test]\n",
    "    print(\"Split: \", i)\n",
    "    i += 1\n",
    "    nn1.learn(x, y, MSE, 100, 128, 0.01)\n",
    "    print()\n",
    "    y_pred = np.argmax(nn1.predict(x_), axis=1)\n",
    "    y_true = np.argmax(y_, axis=1)\n",
    "    e1.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The error rate of model 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean error of the test-train split: 0.005483333333333507\n",
      "The standard deviation of the test-train split 0.0081249615383705\n"
     ]
    }
   ],
   "source": [
    "e1 = np.array(e1)\n",
    "print(\"The mean error of the test-train split:\", 1 - e1.mean())\n",
    "print(\"The standard deviation of the test-train split\", e1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split:  1\n",
      "Split:  20 Loss: 0.00023069723503835212\n",
      "Split:  30 Loss: 2.6849426283975983e-05\n",
      "Split:  40 Loss: 7.108218552595879e-06\n",
      "Split:  50 Loss: 2.971821147846548e-06\n",
      "CPU times: user 1h 9min 35s, sys: 1h 9min 26s, total: 2h 19min 1s\n",
      "Wall time: 12min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 1\n",
    "for train, test in kf.split(X):\n",
    "    x = X[train]; y = Y[train]\n",
    "    x_ = X[test]; y_ = Y[test]\n",
    "    print(\"Split: \", i)\n",
    "    i += 1\n",
    "    nn2.learn(x, y, MSE, 100, 128, 0.01)\n",
    "    print()\n",
    "    y_pred = np.argmax(nn2.predict(x_), axis=1)\n",
    "    y_true = np.argmax(y_, axis=1)\n",
    "    e2.append(accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The error rate of model 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean error of the test-train split: 0.007016666666666671\n",
      "The standard deviation of the test-train split 0.009158086893863566\n"
     ]
    }
   ],
   "source": [
    "e2 = np.array(e2)\n",
    "print(\"The mean error of the test-train split:\", 1 - e2.mean())\n",
    "print(\"The standard deviation of the test-train split\", e2.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. accuracy of Model 1: 0.9945166666666665 \n",
      "Avg. accuracy of Model 2: 0.9929833333333333\n"
     ]
    }
   ],
   "source": [
    "e1 = np.array(e1); e2 = np.array(e2)\n",
    "print(\"Avg. accuracy of Model 1:\", e1.mean(), \"\\nAvg. accuracy of Model 2:\", e2.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model one just, barely, performs better. We therefore choose, the first model to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Digit classifier\n",
    "We have determined that the neural network #2 is the better one to perform classification. We will now train it to on the entire dataset.\n",
    "\n",
    "We use each pixel as a feature to train the network. This results in a network that takes $28\\times28$ number of pixels as input. We have two hidden layers each with 64 nuerons, activated by a Sigmoid function. Lastly, the output layer has 10 neuron which determine the class label of a given input image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 25min 32s, sys: 1h 20min 53s, total: 2h 46min 25s\n",
      "Wall time: 15min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Choose the better model\n",
    "if e1.mean() < e2.mean(): \n",
    "    nn_simple = deepcopy(nn1)\n",
    "else:\n",
    "    nn_simple = deepcopy(nn2)\n",
    "\n",
    "# Train the network\n",
    "nn_simple.learn(img_train, lbl_train, MSE, 500, 128, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some metrics to guage the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics of Performance\n",
      "Accuracy:  97.67 %\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "\n",
      "[[ 967    0    2    0    0    4    6    2    4    4]\n",
      " [   0 1122    2    0    0    0    3    3    1    2]\n",
      " [   1    3 1009    5    3    0    3    9    2    0]\n",
      " [   1    3    6  990    1    7    1    4    6    1]\n",
      " [   0    0    3    0  957    1    4    0    3   12]\n",
      " [   4    1    0    5    0  868    2    0    6    6]\n",
      " [   2    2    2    0    4    2  938    0    1    0]\n",
      " [   1    1    4    5    1    1    0 1000    5    5]\n",
      " [   3    2    4    2    1    5    1    2  942    5]\n",
      " [   1    1    0    3   15    4    0    8    4  974]]\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "Other metrics\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.98       989\n",
      "          1       0.99      0.99      0.99      1133\n",
      "          2       0.98      0.97      0.98      1035\n",
      "          3       0.98      0.97      0.98      1020\n",
      "          4       0.97      0.98      0.98       980\n",
      "          5       0.97      0.97      0.97       892\n",
      "          6       0.98      0.99      0.98       951\n",
      "          7       0.97      0.98      0.98      1023\n",
      "          8       0.97      0.97      0.97       967\n",
      "          9       0.97      0.96      0.96      1010\n",
      "\n",
      "avg / total       0.98      0.98      0.98     10000\n",
      "\n",
      "CPU times: user 613 ms, sys: 709 ms, total: 1.32 s\n",
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Mak epredictions on the test set\n",
    "y_pred = np.argmax(nn_simple.predict(img_test), axis=1)\n",
    "# Get the true labels\n",
    "y_true = np.argmax(lbl_test, axis=1)\n",
    "print(\"Metrics of Performance\")\n",
    "print(\"Accuracy: \", accuracy_score(y_true, y_pred) * 100, \"%\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"\\n\\nConfusion Matrix\\n\")\n",
    "print(confusion_matrix(y_pred, y_true))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"\\n\\nOther metrics\\n\")\n",
    "print(classification_report(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have scored an accuracy of about 98%. From the precision column we note that all classes have high precision. This is also evident from the consfusion matrix. \n",
    "\n",
    "With more epochs, it seems like there is a good chance of overfitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Use KNN classifier to learn hand written digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the KNN from sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Perform 1 Nearest Neighbour\n",
    "K = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the input data format for 1-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(lbl_train, axis=1)\n",
    "y_test = np.argmax(lbl_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a 1NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_1nn = KNeighborsClassifier(K, algorithm='brute')\n",
    "clf_1nn.fit(img_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  96.91\n",
      "CPU times: user 3min 22s, sys: 3.5 s, total: 3min 26s\n",
      "Wall time: 21.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc = clf_1nn.score(img_test, y_test)\n",
    "print(\"Accuracy = \", (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics of Performance\n",
      "Accuracy:  96.91 %\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "Confusion Matrix\n",
      "\n",
      "[[ 973    0    7    0    0    1    4    0    6    2]\n",
      " [   1 1129    6    1    7    1    2   14    1    5]\n",
      " [   1    3  992    2    0    0    0    6    3    1]\n",
      " [   0    0    5  970    0   12    0    2   14    6]\n",
      " [   0    1    1    1  944    2    3    4    5   10]\n",
      " [   1    1    0   19    0  860    5    0   13    5]\n",
      " [   3    1    2    0    3    5  944    0    3    1]\n",
      " [   1    0   16    7    5    1    0  992    4   11]\n",
      " [   0    0    3    7    1    6    0    0  920    1]\n",
      " [   0    0    0    3   22    4    0   10    5  967]]\n",
      "-------------------------------------------------------\n",
      "\n",
      "\n",
      "Other metrics\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.98      0.99       993\n",
      "          1       0.99      0.97      0.98      1167\n",
      "          2       0.96      0.98      0.97      1008\n",
      "          3       0.96      0.96      0.96      1009\n",
      "          4       0.96      0.97      0.97       971\n",
      "          5       0.96      0.95      0.96       904\n",
      "          6       0.99      0.98      0.98       962\n",
      "          7       0.96      0.96      0.96      1037\n",
      "          8       0.94      0.98      0.96       938\n",
      "          9       0.96      0.96      0.96      1011\n",
      "\n",
      "avg / total       0.97      0.97      0.97     10000\n",
      "\n",
      "CPU times: user 3min 20s, sys: 3.57 s, total: 3min 24s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "knn_pred = clf_1nn.predict(img_test)\n",
    "print(\"Metrics of Performance\")\n",
    "print(\"Accuracy: \", accuracy_score(y_true, knn_pred) * 100, \"%\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\"\\n\\nConfusion Matrix\\n\")\n",
    "print(confusion_matrix(knn_pred, y_true))\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(\"\\n\\nOther metrics\\n\")\n",
    "print(classification_report(knn_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Oberservations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some general observations**\n",
    "1. Accuracy: The 1NN classifier has obtained worse accuracy than that of the neural network\n",
    "2. Training time: The nearest neighbour takes no time to train, as it is a lazy learner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    "It looks like the neural network performs a tad bit better than the nearest neighbours. Both algorithms perform really well on **class 0** and **class 1**. The reason might be that, these are such numbers where one mostly can't go wrong when writing. The biggest hurdle for the nearest neighbour algorithm is that of **class 8**. \n",
    "\n",
    "It looks like it confused the number *8* with the number *5*, the most. When using a brute force algorithm such as euclidian distance, some *5* might seem like *8*. One such exmaple is an image displayed from the test set above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbour is associated with the dimensionality problem as it looks at all the features before determining the class of a point. It will have to look at 784 sized vector for 60,000 times and only then can it classify a point. \n",
    "\n",
    "The neural network on the other hand would have to do some matix multiplication, which depends on the number of hidden layers. The neural network has learnt some weights, unlike the nearest neighboiur algorithm, which simply stores all the training data and waits until the prediction time, to use them(training data) for classification.\n",
    "\n",
    "We can clearly see this when we attempt to find the class labels of the test set. The neural network takes about 113ms, while the nearest neighbour takes about 22 seconds. (The processing time reported here are subject to change.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
